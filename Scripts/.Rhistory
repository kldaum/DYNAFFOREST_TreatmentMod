rm(list = ls())
External <- T
# ssh kldaum@striker.eri.ucsb.edu
# cd vdl/DYNAFFOR/Scripts
# sbatch BULK.sh
# PROVIDE NOTES ON PURPOSE OF BATCH
user_notes <- "Just a test..."
if (grepl("MacBook", toString(Sys.getenv()))){
platform <- "laptop"
} else {
platform <- "cluster"
}
if (platform == "laptop"){
if (External == F){
## Laptop HD paths
# path <- "/Volumes/LaCie/DYNAFFORE/V4/" # ending in /
# path_params <-  "/Volumes/LaCie/DYNAFFORE/V4/Parameters/" # ending in /
# path_inputs <- "/Volumes/LaCie/DYNAFFORE/V4/sierra/inputs/" # ending in /
# path_outputs <- "/Volumes/LaCie/DYNAFFORE/V4/sierra/outputs/" # ending in /
# path_scripts <- "/Volumes/LaCie/DYNAFFORE/V4/Scripts/" # ending in /
# path_geospa <- "" # ending in /
} else {
## LaCie paths
path <- "/Volumes/LaCie/DYNAFFORE/V4/" # ending in /
path_params <-  "/Volumes/LaCie/DYNAFFORE/V4/Parameters/" # ending in /
path_inputs <- "/Volumes/LaCie/DYNAFFORE/V4/sierra/inputs/" # ending in /
path_outputs <- "/Volumes/LaCie/DYNAFFORE/V4/sierra/outputs/" # ending in /
path_scripts <- "/Volumes/LaCie/DYNAFFORE/V4/Scripts/" # ending in /
path_geospa <- "" # ending in /
}
}
if (platform == "cluster"){
## Cluster paths
path_inputs <- "/home/kldaum/vdl/scratch-vdl/kldaum1/DYNAFFOR/"
path_outputs <- "/home/kldaum/vdl/scratch-vdl/kldaum1/DYNAFFOR/sierra/outputs/"
path_scripts <- "/home/kldaum/vdl/DYNAFFOR/Scripts/"
path <- path_inputs
}
#
# #### Load libraries
# library(igraph)
# library(sf)
# library(sp)
# library(dbplyr)
# library(DBI)
# library(tidyverse)
# library(raster)
# library(ncdf4)
# library(rgdal)
# library(rgeos)
# library(lwgeom)
# library(RColorBrewer)
# library(rnaturalearth)
# library(solaR)
# library(landscapeR)
# library(landscapemetrics)
# library(spex)
# library(reshape2)
#### Supplemental scripts
# Run supplemental scripts in package
KEditsV4 <- T
module_name <- "Init"
source(paste0(path_scripts, "module_V4KE1.R"))
run_schedule <- read.csv(paste0(path_params, "Params_V4KEdits.csv"))
varis <- names(run_schedule[grep("P_", names(run_schedule))])
run_schedule$Run_file <- 0
run_schedule$Run_path <- 0
run_schedule$Time_ini <- 0
run_schedule$Time_fin <- 0
run_schedule$Time_mins <- 0
bulk_time <- format(Sys.time(), format = "%Y%m%d%H%M%S")
run_time <- bulk_time
path_save <- paste0(path_outputs, "BATCH_", bulk_time, "_KEditsV4/")
dir.create(path_save)
fileConn<-file("user_notes.txt")
writeLines(user_notes, fileConn)
close(fileConn)
for (r in c(1:nrow(run_schedule))){
cat(paste0("\n\n\n\nInitiating run ", run_schedule$Run_ID[r]," (", run_schedule$Run_name[r], ") at ", Sys.time(), "\n\n\n\n\n"))
region <- tolower(run_schedule$Region[r])
run_schedule$Time_ini[r] <- as.character(Sys.time())
run <- run_schedule$Run_ID[r]
n.years <- run_schedule$Years[r]
run_name <- run_schedule$Run_name[r]
for (v in varis){
assign(v, run_schedule[r,v])
}
fire.years <- P_fireyrs
fire.size.mult <- P_firesize_mult
fire.freq.mult <- P_firefreq_mult
climate.yr.range <- as.numeric(unlist(strsplit(P_climate_yrs, ", "))[1]):as.numeric(unlist(strsplit(P_climate_yrs, ", "))[2])
source(paste0(path_scripts, "model_7-29-2021_V4KE1.R"))
filesave <- paste0("output.RUN", sprintf("%02d",r), ".sqlite")
run_schedule$Run_file[r] <- filesave
file.copy(paste0(path_outputs, "output.sqlite"), paste0(path_save, filesave))
if (filesave %in% list.files(path_save)){
cat(paste0(filesave, " successfully copied to output drive\n\n"))
file.remove(paste0(path_inputs, region, "/outputs/output.sqlite"))
run_schedule$Run_path[r] <- paste0(path_save, "output.RUN", sprintf("%02d",r), ".sqlite")
} else {
cat(paste0("\n!! ", filesave, " NOT copied to output drive !!\n\n"))
file.rename(paste0(path_inputs, region, "/outputs/output.sqlite"), paste0(path_inputs, filesave))
run_schedule$Run_path[r] <- paste0(path_inputs, region, "/outputs/", filesave)
}
# Save log_mgmt
if (exists("log_burn")){
if ((KEditsV4 == T) & (exists("path_save") == T)){
write.csv(log_burn, paste0(path_save, "log_burn_", run_name, ".csv"), row.names = F)
} else {
write.csv(log_burn, paste0(path, region, "/outputs/log_burn_", bulk_time, ".csv"), row.names = F)
}
}
# Save raster array means and standard deviations
arrays <- c("list.biomass_dead", "list.biomass_live") # beginning with "list.", ex) list.biomass_dead
for (i in c(1:length(arrays))){
name <- gsub("_", "", unlist(strsplit(arrays[i], ".", fixed = T))[2])
brk <- brick(stack(lapply(get(arrays[i]), raster)))
brk.mean <- as.matrix(calc(brk, mean))
brk.sd <- as.matrix(calc(brk, sd))
write.csv(brk.mean, paste0(path_save, "Run", sprintf("%02d", run), "_ras_", name, "_mean.csv"))
write.csv(brk.sd, paste0(path_save, "Run", sprintf("%02d", run), "_ras_", name, "_sd.csv"))
}
run_schedule$Time_fin[r] <- as.character(Sys.time())
run_schedule$Time_mins <- as.numeric(difftime(time1 = as.POSIXct(run_schedule$Time_fin[r]), time2 = as.POSIXct(run_schedule$Time_ini[r]), units = "mins"))
}
write.csv(run_schedule, paste0(path_save, "run_schedule_", bulk_time, ".csv"), row.names = F)
###
if (run_schedule$Run_file[nrow(run_schedule)] != "0"){
# Time series interval
interval <- ceiling(min(run_schedule$Years)/20)
# Create PFT index
pft_names <- c("Grass/Shrub", # 0
"Aspen", # 1
"California mixed conifer", # 2
"Douglas-fir coastal", # 3
"Douglas-fir inland", # 4
"Englemann Spruce/Fir", # 5
"Five-needle Pine", # 6
"Hemlock/Cedar", # 7
"Lodgepole Pine", # 8
"Mixed Fir", # 9
"Pinyon Pine", # 10
"Ponderosa Pine northern", # 11
"Ponderosa Pine southern") # 12
pft_names <- data.frame(cbind(c(0:(length(pft_names)-1)), pft_names))
names(pft_names) <- c("ID", "Name")
pft_names$ID <- as.numeric(pft_names$ID)
# Generate blank data frames and vectors to fill
sens.df <- data.frame()
sens.ras <- data.frame()
run_names <- vector()
path_PP <- path_save
file_PP <- unlist(strsplit(path_PP, "/"))[length(unlist(strsplit(path_PP, "/")))]
run_schedule <- read.csv(list.files(path_PP, full.names = T)[grepl("run_schedule", list.files(path_PP))])
if (platform == "laptop"){
run_schedule$Run_path <- gsub("home", "Volumes", run_schedule$Run_path)
}
for (f in c(1:nrow(run_schedule))){
file <- run_schedule$Run_file[f]
cat(paste0("\n\nPostprocessing: Run ", sprintf("%02d", run_schedule$Run_ID[f]), "\n\n"))
run_name <- run_schedule$Run_name[f]
run_years <- run_schedule$Years[f]
interval_years <- seq(from = 0, to = run_years, by = interval)
interval_years[1] <- 1
path_output_db <- run_schedule$Run_path[f]
output.db <- DBI::dbConnect(RSQLite::SQLite(), dbname=path_output_db)
#dbListTables(db.conn) # List tables
# Get fuel/fire data
dataset <- tbl(output.db, "Dead_fuel_fire")
data.mort <- dataset%>%filter(year %in% interval_years)
data.mort <- data.frame(data.mort)
# Get stand data
dataset <- tbl(output.db, "Stand")
data.stand <- dataset%>%filter(year %in% interval_years)
data.stand <- data.frame(data.stand)
data <- cbind(data.stand, data.mort)
data <- data[,which(duplicated(names(data)) == F)]
# Substitute 0 for NA in applicable columns
data$stand.density.ha[is.na(data$stand.density.ha)] <- 0
data$avail.litter.kg[is.na(data$avail.litter.kg)] <- 0
data$avail.cwd.kg[is.na(data$avail.cwd.kg)] <- 0
data$ck[is.na(data$ck)] <- 0
# Remove rows with pft = NA
data <- data[which(is.na(data$pft) == F),]
# Add total biomass columns
data$biomass.sum <- (data$stem.biomass.kg + data$leaf.biomass.kg + data$branch.biomass.kg)
data$biomass_dead.sum <- (data$litter.kg + data$cwd.kg + data$snag.kg)
#dbDisconnect()
# Build mortality dataset
data_grouped <- data %>% dplyr::group_by(year, pft) %>% summarize(
coverage = length(x),
mortality = sum(death),
mortality_fire = sum(fire.death),
burned_area = length(which(fire.sev != 0)),
age_mean = mean(stand_age),
age_sd = sd(stand_age),
biomass.mean = mean(biomass.sum),
biomass.sd = sd(biomass.sum),
biomass_dead.mean = mean(biomass_dead.sum),
biomass_dead.sd = sd(biomass_dead.sum)
)
data_grouped <- data.frame(cbind(run_name, file, file_PP, data_grouped))
names(data_grouped)[1:3] <- c("treatment", "filename", "directory")
sens.df <- rbind(sens.df, data_grouped)
data$cell_ID <- match(paste0(data$x, "_", data$y), unique(paste0(data$x, "_", data$y)))
data.ras <- data %>% group_by(cell_ID) %>% summarize( treatment = run_name,
file = file,
directory = file_PP,
cell_ID = cell_ID,
x = x,
y = y,
fire_sev.sum = sum(fire.sev),
fire_death.sum = sum(fire.death),
death.sum = sum(death),
psi.sum = sum(psi),
psi.mean = mean(psi),
psi.sd = sd(psi),
stressed.sum = sum(stressed),
stressed.mean = mean(stressed),
stressed.sd = sd(stressed),
death_prob.sum = sum(death_prob),
death_prob.mean = mean(death_prob),
death_prob.sd = sd(death_prob),
soil_moisture.sum = sum(soil.moisture),
soil_moisture.mean = mean(soil.moisture),
soil_moisture.sd = sd(soil.moisture)
)
data.ras <- data.ras[(duplicated(data.ras) == F),]
# Create log of cell switches, add as column
temp.df <- data %>% dplyr::select(year, cell_ID, pft)
temp.df <- temp.df[order(temp.df$cell_ID),]
temp.df$Diff <- c(0, ((diff(temp.df$pft) != 0) + 0))
temp.df <- data.frame(temp.df %>% group_by(cell_ID) %>% summarize(sum = sum(Diff)))
data.ras$sum_PFTchange <- temp.df$sum[match(data.ras$cell_ID, temp.df$cell_ID)]
# Log initial and final PFT maps
#! temp.df <- data %>% subset(year %in% interval_years) %>% dplyr::select(pft)
sens.ras <- data.frame(rbind(sens.ras, data.ras))
rm("data", "data.stand", "data.mort")
}
# Back up
sens.df.bu <- sens.df
write.csv(sens.df.bu, paste0(path_PP, "sensitivity_BACKUP_", run_time, ".csv"), row.names = F)
# Extract treatment information
sens.df$treatment_type <- unlist(strsplit(sens.df$treatment, "_"))[(c(1:(nrow(sens.df)*3)) %% 3 == T)]
sens.df$treatment_val <- unlist(strsplit(sens.df$treatment, "_"))[(c(3:((nrow(sens.df)*3) + 3)) %% 3 == T)]
sens.df$treatment_rep <- unlist(strsplit(sens.df$treatment, "_"))[(c(2:((nrow(sens.df)*3) + 2)) %% 3 == T)]
# Look up PFT names
sens.df$pft_name <- pft_names$Name[match(sens.df$pft, pft_names$ID)]
# Original value reference
cov_orig <- sens.df[which((sens.df$year == 1) & (sens.df$treatment == sens.df$treatment[1])), c("pft", "coverage")]
cov_orig <- rbind(cov_orig, setNames(data.frame(cbind(c(0:max(sens.df$pft))[which(c(0:max(sens.df$pft)) %in% cov_orig$pft == F)], 1)), names(cov_orig)))
cov_orig <- cov_orig[order(cov_orig$pft),]
# Generate values proportional to pft coverage / original pft coverage
sens.df$mortality.by_area_orig <- sens.df$mortality/cov_orig$coverage[match(sens.df$pft, cov_orig$pft)]
sens.df$mortality.by_area <- sens.df$mortality/sens.df$coverage
sens.df$mortality_fire.by_area_orig <- sens.df$mortality_fire/cov_orig$coverage[match(sens.df$pft, cov_orig$pft)]
sens.df$mortality_fire.by_area <- sens.df$mortality_fire/sens.df$coverage
sens.df$burned_area.by_area_orig <- sens.df$burned_area/cov_orig$coverage[match(sens.df$pft, cov_orig$pft)]
sens.df$burned_area.by_area <- sens.df$burned_area/sens.df$coverage
sens.df$coverage.perc_orig <- (sens.df$coverage/cov_orig$coverage[match(sens.df$pft, cov_orig$pft)])*100
# Track values cumulatively
sens.df <- data.frame(sens.df %>% group_by(treatment, pft) %>% mutate(burned_area.cumu = cumsum(burned_area)))
sens.df <- data.frame(sens.df %>% group_by(treatment, pft) %>% mutate(mortality.cumu = cumsum(mortality)))
sens.df <- data.frame(sens.df %>% group_by(treatment, pft) %>% mutate(mortality_fire.cumu = cumsum(mortality_fire)))
row.names(sens.df) <- c(1:nrow(sens.df))
# Resize
minyr <- min(data.frame(sens.df %>% group_by(treatment) %>% summarize(maxyear = max(year)))[,2])
sens.df <- sens.df %>% subset(year <= minyr)
# Save output tables
write.csv(sens.df, paste0(path_PP, "sensitivity_", run_time, ".csv"), row.names = F)
write.csv(sens.ras, paste0(path_PP, "sensitivity_raster_", run_time, ".csv"), row.names = F)
# Generic plots
source(paste0(path_scripts, "Sensitivity_plots_V4KE1.R"))
}
# Move log to save
if (platform == "cluster"){
logs <- list.files(path_scripts, full.names = F)[grep(".log", list.files(path_scripts))]
logs_paths <- list.files(path_scripts, full.names = T)[grep(".log", list.files(path_scripts))]
cat(paste0("\nMoving Slurm log: ", logs[length(logs)], "\nTo: ", path_save, "\n\n"))
file.rename(logs_paths[length(logs)], paste0(path_save, logs[length(logs)]))
}
# User end
cat("\nEND OF SIMULATION\n\n")
